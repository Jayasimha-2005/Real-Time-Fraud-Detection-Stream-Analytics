{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f8a403",
   "metadata": {},
   "source": [
    "# Phase 2: Deep Learning Models (LSTM + GRU)\\n\n",
    "\\n\n",
    "## Goals:\\n\n",
    "- Prepare sequential data for LSTM/GRU\\n\n",
    "- Build and train LSTM model\\n\n",
    "- Build and train GRU model\\n\n",
    "- Compare performance with baseline models\\n\n",
    "- Save trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3138ecd8",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b59f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\\n\n",
    "import pandas as pd\\n\n",
    "import matplotlib.pyplot as plt\\n\n",
    "import seaborn as sns\\n\n",
    "from sklearn.metrics import (\\n\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\\n\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\\n\n",
    ")\\n\n",
    "import tensorflow as tf\\n\n",
    "from tensorflow import keras\\n\n",
    "from tensorflow.keras.models import Sequential, Model\\n\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Input\\n\n",
    "from tensorflow.keras.optimizers import Adam\\n\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\n\n",
    "import warnings\\n\n",
    "warnings.filterwarnings('ignore')\\n\n",
    "\\n\n",
    "# Set random seeds\\n\n",
    "np.random.seed(42)\\n\n",
    "tf.random.set_seed(42)\\n\n",
    "\\n\n",
    "print(f'TensorFlow version: {tf.__version__}')\\n\n",
    "print(f'Keras version: {keras.__version__}')\\n\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c46ea7",
   "metadata": {},
   "source": [
    "## Step 2: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data from Phase 1\\n\n",
    "X_train = pd.read_csv('../../data/processed/X_train.csv')\\n\n",
    "X_test = pd.read_csv('../../data/processed/X_test.csv')\\n\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv').values.ravel()\\n\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv').values.ravel()\\n\n",
    "\\n\n",
    "print(f'Training set: {X_train.shape}')\\n\n",
    "print(f'Test set: {X_test.shape}')\\n\n",
    "print(f'Training labels: {y_train.shape}')\\n\n",
    "print(f'Test labels: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4724a7fe",
   "metadata": {},
   "source": [
    "##  Step 3: Prepare Sequential Data\\n\n",
    "\\n\n",
    "LSTM and GRU expect 3D input: (samples, time_steps, features)\\n\n",
    "We'll reshape our data to add a time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b66164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for LSTM/GRU\\n\n",
    "# Convert to numpy arrays\\n\n",
    "X_train_seq = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\\n\n",
    "X_test_seq = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\\n\n",
    "\\n\n",
    "print(f'Sequential training data shape: {X_train_seq.shape}')\\n\n",
    "print(f'Sequential test data shape: {X_test_seq.shape}')\\n\n",
    "print(f'Format: (samples, time_steps, features)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c1f82",
   "metadata": {},
   "source": [
    "## Step 4: Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM architecture\\n\n",
    "def build_lstm_model(input_shape):\\n\n",
    "    model = Sequential([\\n\n",
    "        LSTM(64, input_shape=input_shape, return_sequences=True),\\n\n",
    "        Dropout(0.3),\\n\n",
    "        LSTM(32, return_sequences=False),\\n\n",
    "        Dropout(0.3),\\n\n",
    "        Dense(16, activation='relu'),\\n\n",
    "        Dropout(0.2),\\n\n",
    "        Dense(1, activation='sigmoid')\\n\n",
    "    ])\\n\n",
    "    \\n\n",
    "    model.compile(\\n\n",
    "        optimizer=Adam(learning_rate=0.001),\\n\n",
    "        loss='binary_crossentropy',\\n\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\\n\n",
    "    )\\n\n",
    "    \\n\n",
    "    return model\\n\n",
    "\\n\n",
    "# Create model\\n\n",
    "lstm_model = build_lstm_model((X_train_seq.shape[1], X_train_seq.shape[2]))\\n\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3b1e5",
   "metadata": {},
   "source": [
    "## Step 5: Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d999c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\\n\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\\n\n",
    "checkpoint = ModelCheckpoint('../../models/saved_models/lstm_model.keras', \\n\n",
    "                            save_best_only=True, monitor='val_loss')\\n\n",
    "\\n\n",
    "# Calculate class weights (handling imbalance)\\n\n",
    "from sklearn.utils.class_weight import compute_class_weight\\n\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\\n\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\\n\n",
    "\\n\n",
    "print(f'Class weights: {class_weight_dict}')\\n\n",
    "print('\\\\nTraining LSTM model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c57e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\\n\n",
    "history_lstm = lstm_model.fit(\\n\n",
    "    X_train_seq, y_train,\\n\n",
    "    validation_split=0.2,\\n\n",
    "    epochs=20,\\n\n",
    "    batch_size=256,\\n\n",
    "    class_weight=class_weight_dict,\\n\n",
    "    callbacks=[early_stop, checkpoint],\\n\n",
    "    verbose=1\\n\n",
    ")\\n\n",
    "\\n\n",
    "print('\\\\nLSTM model training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a269d",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589db94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\\n\n",
    "y_pred_proba_lstm = lstm_model.predict(X_test_seq).ravel()\\n\n",
    "y_pred_lstm = (y_pred_proba_lstm > 0.5).astype(int)\\n\n",
    "\\n\n",
    "# Metrics\\n\n",
    "print('=== LSTM MODEL RESULTS ===')\\n\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_lstm):.4f}')\\n\n",
    "print(f'Precision: {precision_score(y_test, y_pred_lstm):.4f}')\\n\n",
    "print(f'Recall: {recall_score(y_test, y_pred_lstm):.4f}')\\n\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred_lstm):.4f}')\\n\n",
    "print(f'ROC-AUC: {roc_auc_score(y_test, y_pred_proba_lstm):.4f}')\\n\n",
    "print('\\\\nClassification Report:')\\n\n",
    "print(classification_report(y_test, y_pred_lstm, target_names=['Normal', 'Fraud']))\\n\n",
    "print('\\\\nConfusion Matrix:')\\n\n",
    "print(confusion_matrix(y_test, y_pred_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc50811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\\n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\n",
    "\\n\n",
    "# Loss\\n\n",
    "axes[0, 0].plot(history_lstm.history['loss'], label='Training Loss')\\n\n",
    "axes[0, 0].plot(history_lstm.history['val_loss'], label='Validation Loss')\\n\n",
    "axes[0, 0].set_title('LSTM - Loss', fontsize=12, fontweight='bold')\\n\n",
    "axes[0, 0].set_xlabel('Epoch')\\n\n",
    "axes[0, 0].set_ylabel('Loss')\\n\n",
    "axes[0, 0].legend()\\n\n",
    "axes[0, 0].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "# Accuracy\\n\n",
    "axes[0, 1].plot(history_lstm.history['accuracy'], label='Training Accuracy')\\n\n",
    "axes[0, 1].plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\\n\n",
    "axes[0, 1].set_title('LSTM - Accuracy', fontsize=12, fontweight='bold')\\n\n",
    "axes[0, 1].set_xlabel('Epoch')\\n\n",
    "axes[0, 1].set_ylabel('Accuracy')\\n\n",
    "axes[0, 1].legend()\\n\n",
    "axes[0, 1].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "# Precision\\n\n",
    "axes[1, 0].plot(history_lstm.history['precision'], label='Training Precision')\\n\n",
    "axes[1, 0].plot(history_lstm.history['val_precision'], label='Validation Precision')\\n\n",
    "axes[1, 0].set_title('LSTM - Precision', fontsize=12, fontweight='bold')\\n\n",
    "axes[1, 0].set_xlabel('Epoch')\\n\n",
    "axes[1, 0].set_ylabel('Precision')\\n\n",
    "axes[1, 0].legend()\\n\n",
    "axes[1, 0].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "# Recall\\n\n",
    "axes[1, 1].plot(history_lstm.history['recall'], label='Training Recall')\\n\n",
    "axes[1, 1].plot(history_lstm.history['val_recall'], label='Validation Recall')\\n\n",
    "axes[1, 1].set_title('LSTM - Recall', fontsize=12, fontweight='bold')\\n\n",
    "axes[1, 1].set_xlabel('Epoch')\\n\n",
    "axes[1, 1].set_ylabel('Recall')\\n\n",
    "axes[1, 1].legend()\\n\n",
    "axes[1, 1].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e218f",
   "metadata": {},
   "source": [
    "## Step 7: Build GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ab1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build GRU architecture\\n\n",
    "def build_gru_model(input_shape):\\n\n",
    "    model = Sequential([\\n\n",
    "        GRU(64, input_shape=input_shape, return_sequences=True),\\n\n",
    "        Dropout(0.3),\\n\n",
    "        GRU(32, return_sequences=False),\\n\n",
    "        Dropout(0.3),\\n\n",
    "        Dense(16, activation='relu'),\\n\n",
    "        Dropout(0.2),\\n\n",
    "        Dense(1, activation='sigmoid')\\n\n",
    "    ])\\n\n",
    "    \\n\n",
    "    model.compile(\\n\n",
    "        optimizer=Adam(learning_rate=0.001),\\n\n",
    "        loss='binary_crossentropy',\\n\n",
    "        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\\n\n",
    "    )\\n\n",
    "    \\n\n",
    "    return model\\n\n",
    "\\n\n",
    "# Create model\\n\n",
    "gru_model = build_gru_model((X_train_seq.shape[1], X_train_seq.shape[2]))\\n\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3e644",
   "metadata": {},
   "source": [
    "## Step 8: Train GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c2fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\\n\n",
    "early_stop_gru = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\\n\n",
    "checkpoint_gru = ModelCheckpoint('../../models/saved_models/gru_model.keras', \\n\n",
    "                                save_best_only=True, monitor='val_loss')\\n\n",
    "\\n\n",
    "print('Training GRU model...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f27c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\\n\n",
    "history_gru = gru_model.fit(\\n\n",
    "    X_train_seq, y_train,\\n\n",
    "    validation_split=0.2,\\n\n",
    "    epochs=20,\\n\n",
    "    batch_size=256,\\n\n",
    "    class_weight=class_weight_dict,\\n\n",
    "    callbacks=[early_stop_gru, checkpoint_gru],\\n\n",
    "    verbose=1\\n\n",
    ")\\n\n",
    "\\n\n",
    "print('\\\\nGRU model training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568a643",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\\n\n",
    "y_pred_proba_gru = gru_model.predict(X_test_seq).ravel()\\n\n",
    "y_pred_gru = (y_pred_proba_gru > 0.5).astype(int)\\n\n",
    "\\n\n",
    "# Metrics\\n\n",
    "print('=== GRU MODEL RESULTS ===')\\n\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_gru):.4f}')\\n\n",
    "print(f'Precision: {precision_score(y_test, y_pred_gru):.4f}')\\n\n",
    "print(f'Recall: {recall_score(y_test, y_pred_gru):.4f}')\\n\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred_gru):.4f}')\\n\n",
    "print(f'ROC-AUC: {roc_auc_score(y_test, y_pred_proba_gru):.4f}')\\n\n",
    "print('\\\\nClassification Report:')\\n\n",
    "print(classification_report(y_test, y_pred_gru, target_names=['Normal', 'Fraud']))\\n\n",
    "print('\\\\nConfusion Matrix:')\\n\n",
    "print(confusion_matrix(y_test, y_pred_gru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot GRU training history\\n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\n",
    "\\n\n",
    "# Loss\\n\n",
    "axes[0, 0].plot(history_gru.history['loss'], label='Training Loss')\\n\n",
    "axes[0, 0].plot(history_gru.history['val_loss'], label='Validation Loss')\\n\n",
    "axes[0, 0].set_title('GRU - Loss', fontsize=12, fontweight='bold')\\n\n",
    "axes[0, 0].set_xlabel('Epoch')\\n\n",
    "axes[0, 0].set_ylabel('Loss')\\n\n",
    "axes[0, 0].legend()\\n\n",
    "axes[0, 0].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "# Accuracy\\n\n",
    "axes[0, 1].plot(history_gru.history['accuracy'], label='Training Accuracy')\\n\n",
    "axes[0, 1].plot(history_gru.history['val_accuracy'], label='Validation Accuracy')\\n\n",
    "axes[0, 1].set_title('GRU - Accuracy', fontsize=12, fontweight='bold')\\n\n",
    "axes[0, 1].set_xlabel('Epoch')\\n\n",
    "axes[0, 1].set_ylabel('Accuracy')\\n\n",
    "axes[0, 1].legend()\\n\n",
    "axes[0, 1].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "# Precision\\n\n",
    "axes[1, 0].plot(history_gru.history['precision'], label='Training Precision')\\n\n",
    "axes[1, 0].plot(history_gru.history['val_precision'], label='Validation Precision')\\n\n",
    "axes[1, 0].set_title('GRU - Precision', fontsize=12, fontweight='bold')\\n\n",
    "axes[1, 0].set_xlabel('Epoch')\\n\n",
    "axes[1, 0].set_ylabel('Precision')\\n\n",
    "axes[1, 0].legend()\\n\n",
    "axes[1, 0].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "# Recall\\n\n",
    "axes[1, 1].plot(history_gru.history['recall'], label='Training Recall')\\n\n",
    "axes[1, 1].plot(history_gru.history['val_recall'], label='Validation Recall')\\n\n",
    "axes[1, 1].set_title('GRU - Recall', fontsize=12, fontweight='bold')\\n\n",
    "axes[1, 1].set_xlabel('Epoch')\\n\n",
    "axes[1, 1].set_ylabel('Recall')\\n\n",
    "axes[1, 1].legend()\\n\n",
    "axes[1, 1].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510b130",
   "metadata": {},
   "source": [
    "## Step 10: Compare LSTM vs GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23379fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\\n\n",
    "models = ['LSTM', 'GRU']\\n\n",
    "accuracy = [accuracy_score(y_test, y_pred_lstm), accuracy_score(y_test, y_pred_gru)]\\n\n",
    "precision = [precision_score(y_test, y_pred_lstm), precision_score(y_test, y_pred_gru)]\\n\n",
    "recall = [recall_score(y_test, y_pred_lstm), recall_score(y_test, y_pred_gru)]\\n\n",
    "f1 = [f1_score(y_test, y_pred_lstm), f1_score(y_test, y_pred_gru)]\\n\n",
    "\\n\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\n",
    "\\n\n",
    "# Accuracy\\n\n",
    "axes[0, 0].bar(models, accuracy, color=['#9b59b6', '#1abc9c'])\\n\n",
    "axes[0, 0].set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\\n\n",
    "axes[0, 0].set_ylim([0.9, 1.0])\\n\n",
    "\\n\n",
    "# Precision\\n\n",
    "axes[0, 1].bar(models, precision, color=['#9b59b6', '#1abc9c'])\\n\n",
    "axes[0, 1].set_title('Precision Comparison', fontsize=12, fontweight='bold')\\n\n",
    "axes[0, 1].set_ylim([0, 1])\\n\n",
    "\\n\n",
    "# Recall\\n\n",
    "axes[1, 0].bar(models, recall, color=['#9b59b6', '#1abc9c'])\\n\n",
    "axes[1, 0].set_title('Recall Comparison', fontsize=12, fontweight='bold')\\n\n",
    "axes[1, 0].set_ylim([0, 1])\\n\n",
    "\\n\n",
    "# F1-Score\\n\n",
    "axes[1, 1].bar(models, f1, color=['#9b59b6', '#1abc9c'])\\n\n",
    "axes[1, 1].set_title('F1-Score Comparison', fontsize=12, fontweight='bold')\\n\n",
    "axes[1, 1].set_ylim([0, 1])\\n\n",
    "\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve comparison\\n\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\\n\n",
    "\\n\n",
    "# Plot ROC curves\\n\n",
    "fpr_lstm, tpr_lstm, _ = roc_curve(y_test, y_pred_proba_lstm)\\n\n",
    "fpr_gru, tpr_gru, _ = roc_curve(y_test, y_pred_proba_gru)\\n\n",
    "\\n\n",
    "ax.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {roc_auc_score(y_test, y_pred_proba_lstm):.3f})', linewidth=2)\\n\n",
    "ax.plot(fpr_gru, tpr_gru, label=f'GRU (AUC = {roc_auc_score(y_test, y_pred_proba_gru):.3f})', linewidth=2)\\n\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\\n\n",
    "\\n\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\\n\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\\n\n",
    "ax.set_title('RO Curve Comparison - Deep Learning Models', fontsize=14, fontweight='bold')\\n\n",
    "ax.legend(fontsize=10)\\n\n",
    "ax.grid(alpha=0.3)\\n\n",
    "\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b417c65",
   "metadata": {},
   "source": [
    "## Phase 2 Summary\\n\n",
    "\\n\n",
    "### âœ… Completed:\\n\n",
    "1. Prepared sequential data for LSTM/GRU\\n\n",
    "2. Built and trained LSTM model\\n\n",
    "3. Built and trained GRU model\\n\n",
    "4. Evaluated both models with multiple metrics\\n\n",
    "5. Compared LSTM vs GRU performance\\n\n",
    "6. Saved trained deep learning models\\n\n",
    "\\n\n",
    "### ðŸ”¥ Key Findings:\\n\n",
    "- LSTM and GRU show improved performance over baseline\\n\n",
    "- Both models handle temporal patterns effectively\\n\n",
    "- Ready for autoencoder (Phase 3) and ensemble (Phase 4)\\n\n",
    "\\n\n",
    "### ðŸ“Š Next Steps:\\n\n",
    "- Move to notebook 03/04: Autoencoder & Ensemble\\n\n",
    "- Build unsupervised anomaly detection\\n\n",
    "- Create ensemble fraud scoring system"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
