{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f089a3ec",
   "metadata": {},
   "source": [
    "# Phase 3, 4 & 5: Autoencoder + Ensemble + Evaluation\\n\n",
    "\\n\n",
    "## This notebook covers:\\n\n",
    "1. **Phase 3**: Autoencoder for anomaly detection\\n\n",
    "2. **Phase 4**: Ensemble fraud scoring system\\n\n",
    "3. **Phase 5**: Comprehensive evaluation and reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac2266",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21089a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\\n\n",
    "import pandas as pd\\n\n",
    "import matplotlib.pyplot as plt\\n\n",
    "import seaborn as sns\\n\n",
    "from sklearn.metrics import (\\n\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\\n\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\\n\n",
    "    precision_recall_curve, auc\\n\n",
    ")\\n\n",
    "import tensorflow as tf\\n\n",
    "from tensorflow import keras\\n\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\\n\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\\n\n",
    "from tensorflow.keras.optimizers import Adam\\n\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\\n\n",
    "import joblib\\n\n",
    "import time\\n\n",
    "import warnings\\n\n",
    "warnings.filterwarnings('ignore')\\n\n",
    "\\n\n",
    "# Set random seeds\\n\n",
    "np.random.seed(42)\\n\n",
    "tf.random.set_seed(42)\\n\n",
    "\\n\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db17c7",
   "metadata": {},
   "source": [
    "## Step 2: Load Data and Previous Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8504785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\\n\n",
    "X_train = pd.read_csv('../../data/processed/X_train.csv')\\n\n",
    "X_test = pd.read_csv('../../data/processed/X_test.csv')\\n\n",
    "y_train = pd.read_csv('../../data/processed/y_train.csv').values.ravel()\\n\n",
    "y_test = pd.read_csv('../../data/processed/y_test.csv').values.ravel()\\n\n",
    "\\n\n",
    "print(f'Data loaded: Train {X_train.shape}, Test {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef66db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\\n\n",
    "try:\\n\n",
    "    lstm_model = load_model('../../models/saved_models/lstm_model.keras')\\n\n",
    "    gru_model = load_model('../../models/saved_models/gru_model.keras')\\n\n",
    "    print('LSTM and GRU models loaded successfully!')\\n\n",
    "except:\\n\n",
    "    print('WARNING: LSTM/GRU models not found. Train them in notebook 02 first.')\\n\n",
    "    lstm_model = None\\n\n",
    "    gru_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b0776",
   "metadata": {},
   "source": [
    "# ==== PHASE 3: AUTOENCODER MODEL ====\\n\n",
    "\\n\n",
    "## Step 3: Build Autoencoder\\n\n",
    "\\n\n",
    "Autoencoder trains on **normal transactions only**, then detects fraud as anomalies based on reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4829a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only normal transactions for training\\n\n",
    "X_train_normal = X_train[y_train == 0]\\n\n",
    "X_test_normal = X_test[y_test == 0]\\n\n",
    "X_test_fraud = X_test[y_test == 1]\\n\n",
    "\\n\n",
    "print(f'Normal transactions (training): {X_train_normal.shape}')\\n\n",
    "print(f'Normal transactions (test): {X_test_normal.shape}')\\n\n",
    "print(f'Fraud transactions (test): {X_test_fraud.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186e858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Autoencoder architecture\\n\n",
    "input_dim = X_train.shape[1]\\n\n",
    "encoding_dim = 14\\n\n",
    "\\n\n",
    "# Encoder\\n\n",
    "input_layer = Input(shape=(input_dim,))\\n\n",
    "encoded = Dense(24, activation='relu')(input_layer)\\n\n",
    "encoded = Dropout(0.2)(encoded)\\n\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\\n\n",
    "\\n\n",
    "# Decoder\\n\n",
    "decoded = Dense(24, activation='relu')(encoded)\\n\n",
    "decoded = Dropout(0.2)(decoded)\\n\n",
    "decoded = Dense(input_dim, activation='linear')(decoded)\\n\n",
    "\\n\n",
    "# Autoencoder model\\n\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\\n\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\\n\n",
    "\\n\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c83f6",
   "metadata": {},
   "source": [
    "## Step 4: Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on normal transactions only\\n\n",
    "early_stop_ae = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\\n\n",
    "checkpoint_ae = ModelCheckpoint('../../models/saved_models/autoencoder_model.keras', \\n\n",
    "                               save_best_only=True, monitor='val_loss')\\n\n",
    "\\n\n",
    "print('Training Autoencoder on normal transactions only...')\\n\n",
    "\\n\n",
    "history_ae = autoencoder.fit(\\n\n",
    "    X_train_normal, X_train_normal,\\n\n",
    "    validation_split=0.2,\\n\n",
    "    epochs=30,\\n\n",
    "    batch_size=256,\\n\n",
    "    callbacks=[early_stop_ae, checkpoint_ae],\\n\n",
    "    verbose=1\\n\n",
    ")\\n\n",
    "\\n\n",
    "print('\\\\nAutoencoder training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74149c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\\n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n\n",
    "\\n\n",
    "# Loss\\n\n",
    "axes[0].plot(history_ae.history['loss'], label='Training Loss')\\n\n",
    "axes[0].plot(history_ae.history['val_loss'], label='Validation Loss')\\n\n",
    "axes[0].set_title('Autoencoder - Loss', fontsize=14, fontweight='bold')\\n\n",
    "axes[0].set_xlabel('Epoch')\\n\n",
    "axes[0].set_ylabel('MSE Loss')\\n\n",
    "axes[0].legend()\\n\n",
    "axes[0].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "# MAE\\n\n",
    "axes[1].plot(history_ae.history['mae'], label='Training MAE')\\n\n",
    "axes[1].plot(history_ae.history['val_mae'], label='Validation MAE')\\n\n",
    "axes[1].set_title('Autoencoder - MAE', fontsize=14, fontweight='bold')\\n\n",
    "axes[1].set_xlabel('Epoch')\\n\n",
    "axes[1].set_ylabel('MAE')\\n\n",
    "axes[1].legend()\\n\n",
    "axes[1].grid(alpha=0.3)\\n\n",
    "\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39681ef",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e55004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict (reconstruct)\\n\n",
    "X_test_reconstructed = autoencoder.predict(X_test)\\n\n",
    "\\n\n",
    "# Calculate reconstruction error (MSE per sample)\\n\n",
    "reconstruction_error = np.mean(np.square(X_test.values - X_test_reconstructed), axis=1)\\n\n",
    "\\n\n",
    "print(f'Reconstruction error calculated: {reconstruction_error.shape}')\\n\n",
    "print(f'Mean error: {reconstruction_error.mean():.6f}')\\n\n",
    "print(f'Std error: {reconstruction_error.std():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec5884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction error distribution\\n\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\\n\n",
    "\\n\n",
    "# Plot distributions\\n\n",
    "normal_errors = reconstruction_error[y_test == 0]\\n\n",
    "fraud_errors = reconstruction_error[y_test == 1]\\n\n",
    "\\n\n",
    "ax.hist(normal_errors, bins=50, alpha=0.7, label='Normal', color='#2ecc71', density=True)\\n\n",
    "ax.hist(fraud_errors, bins=50, alpha=0.7, label='Fraud', color='#e74c3c', density=True)\\n\n",
    "ax.set_xlabel('Reconstruction Error', fontsize=12)\\n\n",
    "ax.set_ylabel('Density', fontsize=12)\\n\n",
    "ax.set_title('Reconstruction Error Distribution', fontsize=14, fontweight='bold')\\n\n",
    "ax.legend()\\n\n",
    "ax.grid(alpha=0.3)\\n\n",
    "\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365a801",
   "metadata": {},
   "source": [
    "## Step 6: Set Anomaly Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def96f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold as 95th percentile of normal errors\\n\n",
    "threshold = np.percentile(normal_errors, 95)\\n\n",
    "print(f'Anomaly threshold (95th percentile): {threshold:.6f}')\\n\n",
    "\\n\n",
    "# Predict based on threshold\\n\n",
    "y_pred_ae = (reconstruction_error > threshold).astype(int)\\n\n",
    "\\n\n",
    "# Normalize reconstruction error to [0, 1] for ensemble\\n\n",
    "from sklearn.preprocessing import MinMaxScaler\\n\n",
    "scaler = MinMaxScaler()\\n\n",
    "y_pred_proba_ae = scaler.fit_transform(reconstruction_error.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b448b4b",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\\n\n",
    "print('=== AUTOENCODER RESULTS ===')\\n\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_ae):.4f}')\\n\n",
    "print(f'Precision: {precision_score(y_test, y_pred_ae):.4f}')\\n\n",
    "print(f'Recall: {recall_score(y_test, y_pred_ae):.4f}')\\n\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred_ae):.4f}')\\n\n",
    "print(f'ROC-AUC: {roc_auc_score(y_test, y_pred_proba_ae):.4f}')\\n\n",
    "print('\\\\nClassification Report:')\\n\n",
    "print(classification_report(y_test, y_pred_ae, target_names=['Normal', 'Fraud']))\\n\n",
    "print('\\\\nConfusion Matrix:')\\n\n",
    "print(confusion_matrix(y_test, y_pred_ae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1b6f1c",
   "metadata": {},
   "source": [
    "# ==== PHASE 4: ENSEMBLE FRAUD SCORING ====\\n\n",
    "\\n\n",
    "## Step 8: Generate Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from all models\\n\n",
    "if lstm_model and gru_model:\\n\n",
    "    # Reshape for LSTM/GRU\\n\n",
    "    X_test_seq = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\\n\n",
    "    \\n\n",
    "    y_pred_proba_lstm = lstm_model.predict(X_test_seq).ravel()\\n\n",
    "    y_pred_proba_gru = gru_model.predict(X_test_seq).ravel()\\n\n",
    "    \\n\n",
    "    print('LSTM and GRU predictions generated!')\\n\n",
    "    \\n\n",
    "    # Weighted ensemble\\n\n",
    "    alpha = 0.35  # LSTM weight\\n\n",
    "    beta = 0.35   # GRU weight\\n\n",
    "    gamma = 0.30  # Autoencoder weight\\n\n",
    "    \\n\n",
    "    ensemble_score = (alpha * y_pred_proba_lstm + \\n\n",
    "                     beta * y_pred_proba_gru + \\n\n",
    "                     gamma * y_pred_proba_ae)\\n\n",
    "    \\n\n",
    "    print(f'\\\\nEnsemble weights: LSTM={alpha}, GRU={beta}, Autoencoder={gamma}')\\n\n",
    "    print(f'Ensemble scores generated: {ensemble_score.shape}')\\n\n",
    "else:\\n\n",
    "    print('WARNING: Using Autoencoder only for predictions')\\n\n",
    "    ensemble_score = y_pred_proba_ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6c5d4",
   "metadata": {},
   "source": [
    "## Step 9: Optimize Ensemble Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold using F1-score\\n\n",
    "thresholds = np.linspace(0, 1, 100)\\n\n",
    "f1_scores = []\\n\n",
    "\\n\n",
    "for thresh in thresholds:\\n\n",
    "    y_pred_temp = (ensemble_score > thresh).astype(int)\\n\n",
    "    f1_scores.append(f1_score(y_test, y_pred_temp))\\n\n",
    "\\n\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\\n\n",
    "print(f'Optimal ensemble threshold: {optimal_threshold:.4f}')\\n\n",
    "print(f'Max F1-Score: {max(f1_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot threshold optimization\\n\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\\n\n",
    "ax.plot(thresholds, f1_scores, linewidth=2, color='#3498db')\\n\n",
    "ax.axvline(optimal_threshold, color='#e74c3c', linestyle='--', linewidth=2, \\n\n",
    "          label=f'Optimal Threshold = {optimal_threshold:.3f}')\\n\n",
    "ax.set_xlabel('Threshold', fontsize=12)\\n\n",
    "ax.set_ylabel('F1-Score', fontsize=12)\\n\n",
    "ax.set_title('Ensemble Threshold Optimization', fontsize=14, fontweight='bold')\\n\n",
    "ax.legend()\\n\n",
    "ax.grid(alpha=0.3)\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb39cc",
   "metadata": {},
   "source": [
    "## Step 10: Evaluate Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a891a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final predictions\\n\n",
    "y_pred_ensemble = (ensemble_score > optimal_threshold).astype(int)\\n\n",
    "\\n\n",
    "# Metrics\\n\n",
    "print('=== ENSEMBLE MODEL RESULTS ===')\\n\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_ensemble):.4f}')\\n\n",
    "print(f'Precision: {precision_score(y_test, y_pred_ensemble):.4f}')\\n\n",
    "print(f'Recall: {recall_score(y_test, y_pred_ensemble):.4f}')\\n\n",
    "print(f'F1-Score: {f1_score(y_test, y_pred_ensemble):.4f}')\\n\n",
    "print(f'ROC-AUC: {roc_auc_score(y_test, ensemble_score):.4f}')\\n\n",
    "print('\\\\nClassification Report:')\\n\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Normal', 'Fraud']))\\n\n",
    "print('\\\\nConfusion Matrix:')\\n\n",
    "print(confusion_matrix(y_test, y_pred_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813dd251",
   "metadata": {},
   "source": [
    "# ==== PHASE 5: COMPREHENSIVE EVALUATION ====\\n\n",
    "\\n\n",
    "## Step 11: Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline models for comparison\\n\n",
    "try:\\n\n",
    "    lr_model = joblib.load('../../models/saved_models/logistic_regression.pkl')\\n\n",
    "    rf_model = joblib.load('../../models/saved_models/random_forest.pkl')\\n\n",
    "    \\n\n",
    "    y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\\n\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\\n\n",
    "    \\n\n",
    "    y_pred_lr = lr_model.predict(X_test)\\n\n",
    "    y_pred_rf = rf_model.predict(X_test)\\n\n",
    "    \\n\n",
    "    has_baseline = True\\n\n",
    "    print('Baseline models loaded successfully!')\\n\n",
    "except:\\n\n",
    "    print('WARNING: Baseline models not found')\\n\n",
    "    has_baseline = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c188a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\\n\n",
    "if lstm_model and gru_model and has_baseline:\\n\n",
    "    models_list = ['Logistic Reg', 'Random Forest', 'LSTM', 'GRU', 'Autoencoder', 'Ensemble']\\n\n",
    "    \\n\n",
    "    accuracy_list = [\\n\n",
    "        accuracy_score(y_test, y_pred_lr),\\n\n",
    "        accuracy_score(y_test, y_pred_rf),\\n\n",
    "        accuracy_score(y_test, (y_pred_proba_lstm > 0.5).astype(int)),\\n\n",
    "        accuracy_score(y_test, (y_pred_proba_gru > 0.5).astype(int)),\\n\n",
    "        accuracy_score(y_test, y_pred_ae),\\n\n",
    "        accuracy_score(y_test, y_pred_ensemble)\\n\n",
    "    ]\\n\n",
    "    \\n\n",
    "    precision_list = [\\n\n",
    "        precision_score(y_test, y_pred_lr),\\n\n",
    "        precision_score(y_test, y_pred_rf),\\n\n",
    "        precision_score(y_test, (y_pred_proba_lstm > 0.5).astype(int)),\\n\n",
    "        precision_score(y_test, (y_pred_proba_gru > 0.5).astype(int)),\\n\n",
    "        precision_score(y_test, y_pred_ae),\\n\n",
    "        precision_score(y_test, y_pred_ensemble)\\n\n",
    "    ]\\n\n",
    "    \\n\n",
    "    recall_list = [\\n\n",
    "        recall_score(y_test, y_pred_lr),\\n\n",
    "        recall_score(y_test, y_pred_rf),\\n\n",
    "        recall_score(y_test, (y_pred_proba_lstm > 0.5).astype(int)),\\n\n",
    "        recall_score(y_test, (y_pred_proba_gru > 0.5).astype(int)),\\n\n",
    "        recall_score(y_test, y_pred_ae),\\n\n",
    "        recall_score(y_test, y_pred_ensemble)\\n\n",
    "    ]\\n\n",
    "    \\n\n",
    "    f1_list = [\\n\n",
    "        f1_score(y_test, y_pred_lr),\\n\n",
    "        f1_score(y_test, y_pred_rf),\\n\n",
    "       f1_score(y_test, (y_pred_proba_lstm > 0.5).astype(int)),\\n\n",
    "        f1_score(y_test, (y_pred_proba_gru > 0.5).astype(int)),\\n\n",
    "        f1_score(y_test, y_pred_ae),\\n\n",
    "        f1_score(y_test, y_pred_ensemble)\\n\n",
    "    ]\\n\n",
    "    \\n\n",
    "    # Create comparison DataFrame\\n\n",
    "    results_df = pd.DataFrame({\\n\n",
    "        'Model': models_list,\\n\n",
    "        'Accuracy': accuracy_list,\\n\n",
    "        'Precision': precision_list,\\n\n",
    "        'Recall': recall_list,\\n\n",
    "        'F1-Score': f1_list\\n\n",
    "    })\\n\n",
    "    \\n\n",
    "    print('\\\\n=== COMPREHENSIVE MODEL COMPARISON ===')\\n\n",
    "    print(results_df.to_string(index=False))\\n\n",
    "    \\n\n",
    "    # Save results\\n\n",
    "    results_df.to_csv('../../reports/model_comparison.csv', index=False)\\n\n",
    "    print('\\\\nResults saved to reports/model_comparison.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be60da",
   "metadata": {},
   "source": [
    "## Step 12: Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\\n\n",
    "if lstm_model and gru_model and has_baseline:\\n\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\\n\n",
    "    \\n\n",
    "    # Accuracy\\n\n",
    "    axes[0, 0].barh(models_list, accuracy_list, color='#3498db')\\n\n",
    "    axes[0, 0].set_xlabel('Accuracy', fontsize=12)\\n\n",
    "    axes[0, 0].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\\n\n",
    "    axes[0, 0].set_xlim([0.95, 1.0])\\n\n",
    "    axes[0, 0].grid(alpha=0.3, axis='x')\\n\n",
    "    \\n\n",
    "    # Precision\\n\n",
    "    axes[0, 1].barh(models_list, precision_list, color='#e67e22')\\n\n",
    "    axes[0, 1].set_xlabel('Precision', fontsize=12)\\n\n",
    "    axes[0, 1].set_title('Model Precision Comparison', fontsize=14, fontweight='bold')\\n\n",
    "    axes[0, 1].set_xlim([0, 1])\\n\n",
    "    axes[0, 1].grid(alpha=0.3, axis='x')\\n\n",
    "    \\n\n",
    "    # Recall\\n\n",
    "    axes[1, 0].barh(models_list, recall_list, color='#2ecc71')\\n\n",
    "    axes[1, 0].set_xlabel('Recall', fontsize=12)\\n\n",
    "    axes[1, 0].set_title('Model Recall Comparison', fontsize=14, fontweight='bold')\\n\n",
    "    axes[1, 0].set_xlim([0, 1])\\n\n",
    "    axes[1, 0].grid(alpha=0.3, axis='x')\\n\n",
    "    \\n\n",
    "    # F1-Score\\n\n",
    "    axes[1, 1].barh(models_list, f1_list, color='#9b59b6')\\n\n",
    "    axes[1, 1].set_xlabel('F1-Score', fontsize=12)\\n\n",
    "    axes[1, 1].set_title('Model F1-Score Comparison', fontsize=14, fontweight='bold')\\n\n",
    "    axes[1, 1].set_xlim([0, 1])\\n\n",
    "    axes[1, 1].grid(alpha=0.3, axis='x')\\n\n",
    "    \\n\n",
    "    plt.tight_layout()\\n\n",
    "    plt.savefig('../../reports/model_comparison.png', dpi=300, bbox_inches='tight')\\n\n",
    "    plt.show()\\n\n",
    "    \\n\n",
    "    print('Comparison chart saved to reports/model_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70792e3",
   "metadata": {},
   "source": [
    "## Step 13: ROC & PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\\n\n",
    "if lstm_model and gru_model and has_baseline:\\n\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\\n\n",
    "    \\n\n",
    "    # ROC Curve\\n\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\\n\n",
    "    fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\\n\n",
    "    fpr_lstm, tpr_lstm, _ = roc_curve(y_test, y_pred_proba_lstm)\\n\n",
    "    fpr_gru, tpr_gru, _ = roc_curve(y_test, y_pred_proba_gru)\\n\n",
    "    fpr_ae, tpr_ae, _ = roc_curve(y_test, y_pred_proba_ae)\\n\n",
    "    fpr_ens, tpr_ens, _ = roc_curve(y_test, ensemble_score)\\n\n",
    "    \\n\n",
    "    axes[0].plot(fpr_lr, tpr_lr, label=f'Logistic Reg (AUC={roc_auc_score(y_test, y_pred_proba_lr):.3f})', linewidth=2)\\n\n",
    "    axes[0].plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={roc_auc_score(y_test, y_pred_proba_rf):.3f})', linewidth=2)\\n\n",
    "    axes[0].plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC={roc_auc_score(y_test, y_pred_proba_lstm):.3f})', linewidth=2)\\n\n",
    "    axes[0].plot(fpr_gru, tpr_gru, label=f'GRU (AUC={roc_auc_score(y_test, y_pred_proba_gru):.3f})', linewidth=2)\\n\n",
    "    axes[0].plot(fpr_ae, tpr_ae, label=f'Autoencoder (AUC={roc_auc_score(y_test, y_pred_proba_ae):.3f})', linewidth=2)\\n\n",
    "    axes[0].plot(fpr_ens, tpr_ens, label=f'Ensemble (AUC={roc_auc_score(y_test, ensemble_score):.3f})', linewidth=3, color='red')\\n\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\\n\n",
    "    axes[0].set_xlabel('False Positive Rate', fontsize=12)\\n\n",
    "    axes[0].set_ylabel('True Positive Rate', fontsize=12)\\n\n",
    "    axes[0].set_title('ROC Curve - All Models', fontsize=14, fontweight='bold')\\n\n",
    "    axes[0].legend(fontsize=9)\\n\n",
    "    axes[0].grid(alpha=0.3)\\n\n",
    "    \\n\n",
    "    # Precision-Recall Curve\\n\n",
    "    prec_lr, rec_lr, _ = precision_recall_curve(y_test, y_pred_proba_lr)\\n\n",
    "    prec_rf, rec_rf, _ = precision_recall_curve(y_test, y_pred_proba_rf)\\n\n",
    "    prec_lstm, rec_lstm, _ = precision_recall_curve(y_test, y_pred_proba_lstm)\\n\n",
    "    prec_gru, rec_gru, _ = precision_recall_curve(y_test, y_pred_proba_gru)\\n\n",
    "    prec_ae, rec_ae, _ = precision_recall_curve(y_test, y_pred_proba_ae)\\n\n",
    "    prec_ens, rec_ens, _ = precision_recall_curve(y_test, ensemble_score)\\n\n",
    "    \\n\n",
    "    axes[1].plot(rec_lr, prec_lr, label=f'Logistic Reg (AP={auc(rec_lr, prec_lr):.3f})', linewidth=2)\\n\n",
    "    axes[1].plot(rec_rf, prec_rf, label=f'Random Forest (AP={auc(rec_rf, prec_rf):.3f})', linewidth=2)\\n\n",
    "    axes[1].plot(rec_lstm, prec_lstm, label=f'LSTM (AP={auc(rec_lstm, prec_lstm):.3f})', linewidth=2)\\n\n",
    "    axes[1].plot(rec_gru, prec_gru, label=f'GRU (AP={auc(rec_gru, prec_gru):.3f})', linewidth=2)\\n\n",
    "    axes[1].plot(rec_ae, prec_ae, label=f'Autoencoder (AP={auc(rec_ae, prec_ae):.3f})', linewidth=2)\\n\n",
    "    axes[1].plot(rec_ens, prec_ens, label=f'Ensemble (AP={auc(rec_ens, prec_ens):.3f})', linewidth=3, color='red')\\n\n",
    "    axes[1].set_xlabel('Recall', fontsize=12)\\n\n",
    "    axes[1].set_ylabel('Precision', fontsize=12)\\n\n",
    "    axes[1].set_title('Precision-Recall Curve - All Models', fontsize=14, fontweight='bold')\\n\n",
    "    axes[1].legend(fontsize=9)\\n\n",
    "    axes[1].grid(alpha=0.3)\\n\n",
    "    \\n\n",
    "    plt.tight_layout()\\n\n",
    "    plt.savefig('../../reports/roc_pr_curves.png', dpi=300, bbox_inches='tight')\\n\n",
    "    plt.show()\\n\n",
    "    \\n\n",
    "    print('ROC & PR curves saved to reports/roc_pr_curves.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad67fdb",
   "metadata": {},
   "source": [
    "## Step 14: Latency Measurement (Simulated Real-Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure inference latency\\n\n",
    "print('=== INFERENCE LATENCY MEASUREMENT ===')\\n\n",
    "\\n\n",
    "# Sample 1000 transactions\\n\n",
    "sample_size = 1000\\n\n",
    "X_sample = X_test.iloc[:sample_size]\\n\n",
    "X_sample_seq = X_sample.values.reshape((sample_size, 1, X_sample.shape[1]))\\n\n",
    "\\n\n",
    "latencies = {}\\n\n",
    "\\n\n",
    "if has_baseline:\\n\n",
    "    # Logistic Regression\\n\n",
    "    start = time.time()\\n\n",
    "    _ = lr_model.predict(X_sample)\\n\n",
    "    latencies['Logistic Reg'] = (time.time() - start) * 1000 / sample_size\\n\n",
    "    \\n\n",
    "    # Random Forest\\n\n",
    "    start = time.time()\\n\n",
    "    _ = rf_model.predict(X_sample)\\n\n",
    "    latencies['Random Forest'] = (time.time() - start) * 1000 / sample_size\\n\n",
    "\\n\n",
    "if lstm_model:\\n\n",
    "    # LSTM\\n\n",
    "    start = time.time()\\n\n",
    "    _ = lstm_model.predict(X_sample_seq, verbose=0)\\n\n",
    "    latencies['LSTM'] = (time.time() - start) * 1000 / sample_size\\n\n",
    "\\n\n",
    "if gru_model:\\n\n",
    "    # GRU\\n\n",
    "    start = time.time()\\n\n",
    "    _ = gru_model.predict(X_sample_seq, verbose=0)\\n\n",
    "    latencies['GRU'] = (time.time() - start) * 1000 / sample_size\\n\n",
    "\\n\n",
    "# Autoencoder\\n\n",
    "start = time.time()\\n\n",
    "_ = autoencoder.predict(X_sample, verbose=0)\\n\n",
    "latencies['Autoencoder'] = (time.time() - start) * 1000 / sample_size\\n\n",
    "\\n\n",
    "# Ensemble\\n\n",
    "start = time.time()\\n\n",
    "if lstm_model and gru_model:\\n\n",
    "    _ = lstm_model.predict(X_sample_seq, verbose=0)\\n\n",
    "    _ = gru_model.predict(X_sample_seq, verbose=0)\\n\n",
    "_ = autoencoder.predict(X_sample, verbose=0)\\n\n",
    "latencies['Ensemble'] = (time.time() - start) * 1000 / sample_size\\n\n",
    "\\n\n",
    "print('\\\\nAverage Latency per Transaction (ms):')\\n\n",
    "for model, latency in latencies.items():\\n\n",
    "    print(f'{model}: {latency:.3f} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0255ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latency\\n\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\\n\n",
    "models = list(latencies.keys())\\n\n",
    "lats = list(latencies.values())\\n\n",
    "\\n\n",
    "bars = ax.barh(models, lats, color='#e74c3c')\\n\n",
    "ax.set_xlabel('Latency (ms)', fontsize=12)\\n\n",
    "ax.set_title('Model Inference Latency (per transaction)', fontsize=14, fontweight='bold')\\n\n",
    "ax.grid(alpha=0.3, axis='x')\\n\n",
    "\\n\n",
    "# Add value labels\\n\n",
    "for bar in bars:\\n\n",
    "    width = bar.get_width()\\n\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2, \\n\n",
    "           f'{width:.2f} ms', ha='left', va='center', fontsize=10)\\n\n",
    "\\n\n",
    "plt.tight_layout()\\n\n",
    "plt.savefig('../../reports/latency_comparison.png', dpi=300, bbox_inches='tight')\\n\n",
    "plt.show()\\n\n",
    "\\n\n",
    "print('Latency chart saved to reports/latency_comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2aaec",
   "metadata": {},
   "source": [
    "## Final Summary\\n\n",
    "\\n\n",
    "### ‚úÖ All Phases Completed:\\n\n",
    "\\n\n",
    "**Phase 1**: Data preprocessing + baseline models\\n\n",
    "**Phase 2**: LSTM + GRU deep learning models\\n\n",
    "**Phase 3**: Autoencoder anomaly detection\\n\n",
    "**Phase 4**: Ensemble fraud scoring system\\n\n",
    "**Phase 5**: Comprehensive evaluation + reporting\\n\n",
    "\\n\n",
    "### üî• Key Achievements:\\n\n",
    "- Built 6 fraud detection models\\n\n",
    "- Comprehensive comparison across all metrics\\n\n",
    "- Optimized ensemble scoring system\\n\n",
    "- Measured real-time inference latency\\n\n",
    "- Generated research-grade visualizations\\n\n",
    "\\n\n",
    "### üìä Next Steps:\\n\n",
    "- Implement Kafka + Spark real-time streaming (Phase 4 extension)\\n\n",
    "- Deploy models in production environment\\n\n",
    "- Add explainability (SHAP/LIME)\\n\n",
    "- Create PowerPoint presentation\\n\n",
    "\\n\n",
    "### ‚ö†Ô∏è Limitations:\\n\n",
    "- Simulated environment (not production-grade)\\n\n",
    "- Single-node execution (not distributed)\\n\n",
    "- PCA features (limited interpretability)\\n\n",
    "- Static dataset (not truly streaming)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
